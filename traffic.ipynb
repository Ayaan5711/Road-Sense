{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94210,
     "status": "ok",
     "timestamp": 1722876405627,
     "user": {
      "displayName": "Ayaan Ahmed",
      "userId": "15158203945446334434"
     },
     "user_tz": -330
    },
    "id": "pFTQJByZSkKx",
    "outputId": "73181685-f975-4622-e83e-c7e2718e609c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: opencv-python-headless in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opencv-python-headless) (1.26.4)\n",
      "Requirement already satisfied: openvino in c:\\users\\hp\\anaconda3\\lib\\site-packages (2024.3.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.16.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openvino) (1.26.4)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openvino) (2024.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openvino) (23.2)\n",
      "Requirement already satisfied: openvino-dev in c:\\users\\hp\\anaconda3\\lib\\site-packages (2024.3.0)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openvino-dev) (0.7.1)\n",
      "Requirement already satisfied: networkx<=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openvino-dev) (3.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.16.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openvino-dev) (1.26.4)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openvino-dev) (2024.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openvino-dev) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openvino-dev) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.25.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openvino-dev) (2.32.2)\n",
      "Requirement already satisfied: openvino==2024.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openvino-dev) (2024.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.25.1->openvino-dev) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.25.1->openvino-dev) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.25.1->openvino-dev) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.25.1->openvino-dev) (2024.7.4)\n",
      "Requirement already satisfied: openvino-telemetry in c:\\users\\hp\\anaconda3\\lib\\site-packages (2024.1.0)\n",
      "Requirement already satisfied: supervision in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from supervision) (0.7.1)\n",
      "Requirement already satisfied: matplotlib>=3.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from supervision) (3.8.4)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from supervision) (1.26.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.5.5.64 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from supervision) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=9.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from supervision) (10.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from supervision) (6.0.1)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from supervision) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib>=3.6.0->supervision) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.16.0)\n",
      "Requirement already satisfied: ultralytics in c:\\users\\hp\\anaconda3\\lib\\site-packages (8.2.58)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ultralytics) (3.8.4)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ultralytics) (10.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ultralytics) (2.32.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ultralytics) (2.3.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ultralytics) (0.18.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ultralytics) (4.66.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from ultralytics) (6.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from ultralytics) (2.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.8.0->ultralytics) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.8.0->ultralytics) (2021.13.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: vidgear in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.3.3)\n",
      "Requirement already satisfied: colorlog in c:\\users\\hp\\anaconda3\\lib\\site-packages (from vidgear) (6.8.2)\n",
      "Requirement already satisfied: cython in c:\\users\\hp\\anaconda3\\lib\\site-packages (from vidgear) (3.0.11)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from vidgear) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from vidgear) (2.32.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from vidgear) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from colorlog->vidgear) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->vidgear) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->vidgear) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->vidgear) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->vidgear) (2024.7.4)\n",
      "Requirement already satisfied: yt-dlp in c:\\users\\hp\\anaconda3\\lib\\site-packages (2024.8.1)\n",
      "Requirement already satisfied: brotli in c:\\users\\hp\\anaconda3\\lib\\site-packages (from yt-dlp) (1.0.9)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\anaconda3\\lib\\site-packages (from yt-dlp) (2024.7.4)\n",
      "Requirement already satisfied: mutagen in c:\\users\\hp\\anaconda3\\lib\\site-packages (from yt-dlp) (1.47.0)\n",
      "Requirement already satisfied: pycryptodomex in c:\\users\\hp\\anaconda3\\lib\\site-packages (from yt-dlp) (3.20.0)\n",
      "Requirement already satisfied: requests<3,>=2.32.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from yt-dlp) (2.32.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from yt-dlp) (2.2.2)\n",
      "Requirement already satisfied: websockets>=12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from yt-dlp) (12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.32.2->yt-dlp) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.32.2->yt-dlp) (3.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install opencv-python-headless\n",
    "!pip install openvino\n",
    "!pip install openvino-dev\n",
    "!pip install openvino-telemetry\n",
    "!pip install supervision\n",
    "!pip install ultralytics\n",
    "!pip install vidgear\n",
    "!pip install yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2953,
     "status": "ok",
     "timestamp": 1722876408575,
     "user": {
      "displayName": "Ayaan Ahmed",
      "userId": "15158203945446334434"
     },
     "user_tz": -330
    },
    "id": "_isELGDzTtOT",
    "outputId": "93d5a2cf-3afb-447f-c7b8-d24adb786787"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n",
      "\u001b[32m11:39:10\u001b[0m :: \u001b[1;35m   Helper    \u001b[0m :: \u001b[1;36m  INFO  \u001b[0m :: \u001b[1;37mRunning VidGear Version: 0.3.3\u001b[0m\n",
      "\u001b[32m11:39:10\u001b[0m :: \u001b[1;35m   Helper    \u001b[0m :: \u001b[1;33m DEBUG  \u001b[0m :: \u001b[1;37mSelecting `best` resolution for streams.\u001b[0m\n",
      "\u001b[32m11:39:10\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;36m  INFO  \u001b[0m :: \u001b[1;37mVerifying Streaming URL using yt-dlp backend. Please wait...\u001b[0m\n",
      "\u001b[32m11:39:14\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;36m  INFO  \u001b[0m :: \u001b[1;37m[Backend] :: Streaming URL is fully supported. Available Streams are: [144p, 240p, 360p, 480p, 720p, best, worst]\u001b[0m\n",
      "\u001b[32m11:39:14\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;31m\u001b[2;33mWARNING \u001b[0m :: \u001b[1;37mLivestream URL detected. It is strongly recommended to use the GStreamer backend (`backend=cv2.CAP_GSTREAMER`) with these URLs.\u001b[0m\n",
      "\u001b[32m11:39:14\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;33m DEBUG  \u001b[0m :: \u001b[1;37mUsing `best` resolution for streaming.\u001b[0m\n",
      "\u001b[32m11:39:14\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;33m DEBUG  \u001b[0m :: \u001b[1;37mYouTube source ID: `z545k7Tcb5o`, Title: `Périphérique Nord - Porte de la Pape 2024-08-15 11:39`, Quality: `best`\u001b[0m\n",
      "\u001b[32m11:39:14\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;33m DEBUG  \u001b[0m :: \u001b[1;37mEnabling Threaded Queue Mode for the current video source!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'title', 'formats', 'thumbnails', 'thumbnail', 'description', 'channel_id', 'channel_url', 'view_count', 'average_rating', 'age_limit', 'webpage_url', 'categories', 'tags', 'playable_in_embed', 'live_status', 'release_timestamp', '_format_sort_fields', 'automatic_captions', 'subtitles', 'comment_count', 'chapters', 'heatmap', 'like_count', 'concurrent_view_count', 'channel', 'channel_follower_count', 'uploader', 'uploader_id', 'uploader_url', 'upload_date', 'timestamp', 'availability', 'original_url', 'webpage_url_basename', 'webpage_url_domain', 'extractor', 'extractor_key', 'playlist', 'playlist_index', 'display_id', 'fulltitle', 'release_date', 'release_year', 'is_live', 'was_live', 'requested_subtitles', '_has_drm', 'epoch', 'format_id', 'format_index', 'url', 'manifest_url', 'tbr', 'ext', 'fps', 'protocol', 'preference', 'quality', 'has_drm', 'width', 'height', 'vcodec', 'acodec', 'dynamic_range', 'source_preference', 'resolution', 'aspect_ratio', 'http_headers', 'video_ext', 'audio_ext', 'abr', 'vbr', 'format'])\n",
      "30.0\n",
      "232 - 1280x720\n",
      "None\n",
      "audio only\n",
      "audio only\n",
      "256x144\n",
      "426x240\n",
      "640x360\n",
      "854x480\n",
      "1280x720\n",
      "https://manifest.googlevideo.com/api/manifest/hls_playlist/expire/1723723753/ei/iZu9ZraWGoLF4-EP0oL10AE/ip/45.248.56.249/id/z545k7Tcb5o.1/itag/232/source/yt_live_broadcast/requiressl/yes/ratebypass/yes/live/1/sgovp/gir%3Dyes%3Bitag%3D136/rqh/1/hdlc/1/hls_chunk_host/rr2---sn-hxbicxa5cc5oq-jj06.googlevideo.com/xpc/EgVo2aDSNQ%3D%3D/playlist_duration/3600/manifest_duration/3600/vprv/1/playlist_type/DVR/initcwndbps/852500/mh/SJ/mip/45.248.56.54/mm/44/mn/sn-hxbicxa5cc5oq-jj06/ms/lva/mv/m/mvi/2/pcm2cms/yes/pl/24/dover/13/pacing/0/short_key/1/keepalive/yes/mt/1723701642/sparams/expire,ei,ip,id,itag,source,requiressl,ratebypass,live,sgovp,rqh,hdlc,xpc,playlist_duration,manifest_duration,vprv,playlist_type/sig/AJfQdSswRAIgYJFYajzbytG84YB3DWLoaU6BirsvavfS5gXXzVjAjEUCIEGZSNq26FqG2d79Kiwm2pdH6CO-lPOzvYhKRsHRnzjC/lsparams/hls_chunk_host,initcwndbps,mh,mip,mm,mn,ms,mv,mvi,pcm2cms,pl/lsig/AGtxev0wRAIgPHrUMvO2M3IDZbpPaguQfUd1feEp9B799q5HVWVdQT8CIGWNf1_vVm5e3QoUsXKfSy3r85VMlquJvX1HBt0DEx8Z/playlist/index.m3u8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import\n",
    "from vidgear.gears import CamGear\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "source=\"https://youtu.be/z545k7Tcb5o\"\n",
    "# YouTube Video URL as input source\n",
    "# Stream Mode (`stream_mode = True`)\n",
    "stream = CamGear(\n",
    "    source=source, stream_mode=True, logging=True,  time_delay=0\n",
    ").start()\n",
    "video_metadata=stream.ytv_metadata\n",
    "\n",
    "print(video_metadata.keys())\n",
    "\n",
    "print(video_metadata['fps'])\n",
    "print(video_metadata['format'])\n",
    "print(video_metadata['format_index'])\n",
    "\n",
    "# search available resolution\n",
    "resolutions=[format['resolution'] for format in video_metadata['formats']]\n",
    "for res in resolutions:\n",
    "    print(res)\n",
    "\n",
    "# select the desired resolution to get right url\n",
    "desired_resolution = '1280x720'\n",
    "for format in video_metadata['formats']:\n",
    "\n",
    "    if format['resolution'] == desired_resolution:\n",
    "        VIDEO = format['url']\n",
    "        break\n",
    "\n",
    "print(VIDEO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7982,
     "status": "ok",
     "timestamp": 1722876748561,
     "user": {
      "displayName": "Ayaan Ahmed",
      "userId": "15158203945446334434"
     },
     "user_tz": -330
    },
    "id": "rIRHuq5MTxCS",
    "outputId": "4264a407-21f4-4d94-e6dc-0dacc087c627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoInfo(width=1280, height=720, fps=30, total_frames=-3074457345618259)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: The `frame_resolution_wh` parameter is no longer required and will be dropped in version supervision-0.24.0. The mask resolution is now calculated automatically based on the polygon coordinates.\n",
      "SupervisionWarnings: The `frame_resolution_wh` parameter is no longer required and will be dropped in version supervision-0.24.0. The mask resolution is now calculated automatically based on the polygon coordinates.\n",
      "SupervisionWarnings: The `frame_resolution_wh` parameter is no longer required and will be dropped in version supervision-0.24.0. The mask resolution is now calculated automatically based on the polygon coordinates.\n",
      "SupervisionWarnings: BoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n",
      "SupervisionWarnings: BoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n",
      "SupervisionWarnings: BoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n",
      "SupervisionWarnings: `track_buffer` in `ByteTrack.__init__` is deprecated and will be remove in `supervision-0.23.0`. Use 'lost_track_buffer' instead.\n",
      "SupervisionWarnings: `track_thresh` in `ByteTrack.__init__` is deprecated and will be remove in `supervision-0.23.0`. Use 'track_activation_threshold' instead.\n",
      "SupervisionWarnings: `match_thresh` in `ByteTrack.__init__` is deprecated and will be remove in `supervision-0.23.0`. Use 'minimum_matching_threshold' instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import os\n",
    "from collections import defaultdict, deque\n",
    "# os.environ.pop(\"QT_QPA_PLATFORM_PLUGIN_PATH\")\n",
    "# from supervision import draw_text , Color\n",
    "from ultralytics import YOLO\n",
    "# load yolo model and get class name\n",
    "MODEL = \"models/yolov8s.pt\"\n",
    "# MODEL = \"models/yolov9c.pt\"\n",
    "model=YOLO(MODEL)\n",
    "CLASS_NAMES_DICT = model.model.names\n",
    "print(CLASS_NAMES_DICT)\n",
    "# load openvino model to get faster FPS\n",
    "#model = YOLO(\"models/yolov8s_openvino_model/\", task='detect')\n",
    "# model = YOLO(\"models/yolov9c_openvino_model/\", task='detect')\n",
    "\n",
    "# model=YOLO(MODEL)\n",
    "# model.fuse()\n",
    "\n",
    "colors = sv.ColorPalette.LEGACY\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(VIDEO)\n",
    "print(video_info)\n",
    "# calculate ratio between video stream and displayed size (here's 1280)\n",
    "\n",
    "coef=video_info.width/1280\n",
    "# print(coef)\n",
    "\n",
    "# polygon design\n",
    "#  ----> x\n",
    "# |         (x4,y4)   (x3,y3)\n",
    "# |              +-------+\n",
    "#               +-------+\n",
    "# y            +-------+\n",
    "#         (x1,y1)    (x2,y2)\n",
    "\n",
    "# 3 polygons so 3 values in each coordinate from left to right\n",
    "#    [zone1,zone2, zone3]\n",
    "x1 = [-160 , -25 , 971  ]\n",
    "y1 = [ 405 , 710 , 671  ]\n",
    "x2 = [ 112 , 568 , 1480 ]\n",
    "y2 = [ 503 , 710 , 671  ]\n",
    "x3 = [ 557 , 706 , 874  ]\n",
    "y3 = [ 195 , 212 , 212  ]\n",
    "x4 = [ 411 , 569 , 749  ]\n",
    "y4 = [ 195 , 212 , 212  ]\n",
    "# transform according video stream and displayed video ratio\n",
    "x1, y1, x2, y2, x3, y3, x4, y4 = map(lambda x: [value * coef for value in x], [x1, y1, x2, y2, x3, y3, x4, y4])\n",
    "\n",
    "\n",
    "# search middle point of the polygon (x1+x4)/2) or tier point from top ( x1 + 2* x4) / 3) to draw line for counting\n",
    "x14 = [( x1 + 2 * x4) / 3\n",
    "       for x1,x4\n",
    "       in zip(x1,x4)]\n",
    "y14 = [( y1 + 2 * y4) / 3\n",
    "       for y1,y4\n",
    "       in zip(y1,y4)]\n",
    "x23 = [ ( x2 + 2 * x3) / 3\n",
    "       for x2,x3\n",
    "       in zip(x2,x3)]\n",
    "y23 = [( y2 + 2 * y3) / 3\n",
    "       for y2,y3\n",
    "       in zip(y2,y3)]\n",
    "\n",
    "# polygon zone from left to right (becarefull must be in the same order than le linezone)\n",
    "polygons = [\n",
    "  np.array([\n",
    " [x1, y1],[x2 , y2],[x3 , y3],[x4 , y4]\n",
    "  ],np.int32)\n",
    " for x1, y1, x2, y2, x3, y3, x4, y4\n",
    " in zip(x1, y1, x2, y2, x3, y3, x4, y4)\n",
    "]\n",
    "\n",
    "\n",
    "# initialize our zones\n",
    "\n",
    "zones = [\n",
    "    sv.PolygonZone(\n",
    "        polygon = polygon,\n",
    "        frame_resolution_wh = video_info.resolution_wh\n",
    "    )\n",
    "    for polygon\n",
    "    in polygons\n",
    "]\n",
    "zone_annotators = [\n",
    "    sv.PolygonZoneAnnotator(\n",
    "        zone = zone,\n",
    "        color = colors.by_idx(index),\n",
    "        thickness = 2,\n",
    "        text_thickness = 1,\n",
    "        text_scale = 0.5,\n",
    "    )\n",
    "    for index, zone\n",
    "    in enumerate(zones)\n",
    "]\n",
    "\n",
    "label_annotators = [\n",
    "    sv.LabelAnnotator(\n",
    "        text_position = sv.Position.TOP_CENTER,\n",
    "        color=colors.by_idx(index),\n",
    "        text_thickness = 1,\n",
    "        text_scale = 0.5,\n",
    "        )\n",
    "        for index\n",
    "        in range(len(zones))\n",
    "]\n",
    "\n",
    "# box_annotators = [\n",
    "#     sv.BoxAnnotator(\n",
    "#         color=colors.by_idx(index),\n",
    "#         thickness=1,\n",
    "#         text_thickness=1,\n",
    "#         text_scale=0.5\n",
    "#         )\n",
    "#     for index\n",
    "#     in range(len(polygons))\n",
    "# ]\n",
    "box_annotators = [\n",
    "    sv.BoundingBoxAnnotator(\n",
    "        color = colors.by_idx(index),\n",
    "        thickness = 1,\n",
    "        )\n",
    "    for index\n",
    "    in range(len(polygons))\n",
    "]\n",
    "\n",
    "trace_annotators=[\n",
    "    sv.TraceAnnotator(\n",
    "        color = colors.by_idx(index),\n",
    "        thickness = 1,\n",
    "        trace_length = video_info.fps * 1.5,\n",
    "        position = sv.Position.BOTTOM_CENTER,\n",
    "        )\n",
    "    for index\n",
    "    in range(len(polygons))\n",
    "]\n",
    "\n",
    "\n",
    "lines_start=[\n",
    "\n",
    "    sv.Point(x14, y14)\n",
    "    for x14,y14\n",
    "    in zip(x14,y14)\n",
    "\n",
    "]\n",
    "\n",
    "lines_end =[\n",
    "\n",
    "    sv.Point(x23, y23)\n",
    "    for x23,y23\n",
    "    in zip(x23,y23)\n",
    "]\n",
    "\n",
    "positions=[(sv.Position.CENTER,sv.Position.CENTER),\n",
    "           (sv.Position.CENTER,sv.Position.CENTER),\n",
    "           (sv.Position.CENTER,sv.Position.CENTER),\n",
    "          ]\n",
    "\n",
    "line_zones = [ sv.LineZone(start=line_start, end=line_end, triggering_anchors=position)\n",
    "            for line_start, line_end, position\n",
    "            in zip(lines_start,lines_end,positions)\n",
    "]\n",
    "\n",
    "# for automatic line zone annotator not use here want to use a custom one\n",
    "line_zone_annotators = [sv.LineZoneAnnotator(thickness = 1,\n",
    "                                           color = colors.by_idx(index),\n",
    "                                            text_thickness = 1,\n",
    "                                              text_scale = 0.5,\n",
    "                                                text_offset = 4)\n",
    "    for index\n",
    "    in range(len(line_zones))\n",
    "]\n",
    "\n",
    "# couting line zone text position\n",
    "text_pos=[ sv.Point (x = 100,y = 320),\n",
    "            sv.Point (x = 700,y = 320),\n",
    "            sv.Point (x = 1077,y = 320)\n",
    "\n",
    "]\n",
    "byte_tracker = sv.ByteTrack(track_thresh=0.25, track_buffer=100, match_thresh=0.8, frame_rate=video_info.fps)\n",
    "\n",
    "# byte_tracker = sv.ByteTrack()\n",
    "fps_monitor = sv.FPSMonitor()\n",
    "heat_map = sv.HeatMapAnnotator ()\n",
    "smoother = sv.DetectionsSmoother()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 818,
     "status": "ok",
     "timestamp": 1722876773430,
     "user": {
      "displayName": "Ayaan Ahmed",
      "userId": "15158203945446334434"
     },
     "user_tz": -330
    },
    "id": "47UVD8sIUBpH"
   },
   "outputs": [],
   "source": [
    "SOURCES = np.array([[\n",
    "    [x4[0], y4[0]],\n",
    "    [x3[0], y3[0]],\n",
    "    [x2[0], y2[0]],\n",
    "    [x1[0], y1[0]]\n",
    "\n",
    "],[ [x4[1], y4[1]],\n",
    "    [x3[1], y3[1]],\n",
    "    [x2[1], y2[1]],\n",
    "    [x1[1], y1[1]]\n",
    "],\n",
    "\n",
    "[\n",
    "    [x4[2], y4[2]],\n",
    "    [x3[2], y3[2]],\n",
    "    [x2[2], y2[2]],\n",
    "    [x1[2], y1[2]]\n",
    "]])\n",
    "\n",
    "#zone1 in meters\n",
    "TARGET_WIDTH = 6\n",
    "TARGET_HEIGHT = 75\n",
    "\n",
    "TARGETS = np.array([\n",
    "    [0, 0],\n",
    "    [TARGET_WIDTH - 1, 0],\n",
    "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "    [0, TARGET_HEIGHT - 1],\n",
    "])\n",
    "\n",
    "#zone 2 in meters\n",
    "TARGET_WIDTH = 6\n",
    "TARGET_HEIGHT = 85\n",
    "\n",
    "TARGETS= np.append(TARGETS, np.array([\n",
    "    [0, 0],\n",
    "    [TARGET_WIDTH - 1, 0],\n",
    "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "    [0, TARGET_HEIGHT - 1],\n",
    "]), axis=0)\n",
    "\n",
    "#zone3 in meters\n",
    "TARGET_WIDTH = 6\n",
    "TARGET_HEIGHT = 80\n",
    "\n",
    "\n",
    "TARGETS = np.append(TARGETS, np.array([\n",
    "    [0, 0],\n",
    "    [TARGET_WIDTH - 1, 0],\n",
    "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "    [0, TARGET_HEIGHT - 1],\n",
    "]),axis=0)\n",
    "\n",
    "TARGETS = TARGETS.reshape(3, 4, 2)\n",
    "\n",
    "\n",
    "\n",
    "class ViewTransformer:\n",
    "    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:\n",
    "        source = source.astype(np.float32)\n",
    "        target = target.astype(np.float32)\n",
    "        self.m = cv2.getPerspectiveTransform(source, target)\n",
    "\n",
    "    def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
    "        if points.size == 0:\n",
    "            return points\n",
    "\n",
    "        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
    "        transformed_points = cv2.perspectiveTransform(\n",
    "                reshaped_points, self.m)\n",
    "        return transformed_points.reshape(-1, 2)\n",
    "\n",
    "# create the transformers matrix for each zone\n",
    "view_transformers=[ViewTransformer(source=s, target=t)\n",
    "                  for s,t\n",
    "                  in zip(SOURCES, TARGETS)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1722876778872,
     "user": {
      "displayName": "Ayaan Ahmed",
      "userId": "15158203945446334434"
     },
     "user_tz": -330
    },
    "id": "mYTOwIoqUCYP"
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "selected_classes = [2, 3, 5, 7] # car, motorcycle, bus, truck from coco classes\n",
    "#the dictionary that we will use to store the coordinates for each zone\n",
    "coordinates = defaultdict(lambda: deque(maxlen=30))\n",
    "coordinates = np.append(coordinates,defaultdict(lambda: deque(maxlen=30)))\n",
    "coordinates = np.append(coordinates,defaultdict(lambda: deque(maxlen=30)))\n",
    "\n",
    "def process_frame(frame: np.ndarray, fps) -> np.ndarray:\n",
    "    speed_labels = [],[],[]\n",
    "\n",
    "    results = model(frame, imgsz=640, verbose=False)[0]\n",
    "    # results = model(frame)[0]\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    detections = detections[np.isin(detections.class_id, selected_classes)] # filer on selected classes\n",
    "    detections = byte_tracker.update_with_detections(detections)\n",
    "    # detections = smoother.update_with_detections(detections)\n",
    "\n",
    "    # copy frame before annotate\n",
    "    annotated_frame = frame.copy()\n",
    "\n",
    "    for i, (zone, zone_annotator, box_annotator, trace_annotator, line_zone,\n",
    "            line_zone_annotator,\n",
    "            label_annotator,\n",
    "            line_start,\n",
    "            line_end,\n",
    "            view_transformer,\n",
    "            speed_label,coordinate\n",
    "            ) in  enumerate(zip(zones, zone_annotators, box_annotators,\n",
    "                                trace_annotators,\n",
    "                                line_zones,\n",
    "                                line_zone_annotators,\n",
    "                                label_annotators,\n",
    "                                lines_start,\n",
    "                                lines_end,\n",
    "                                view_transformers,\n",
    "                                speed_labels,\n",
    "                                coordinates\n",
    "                                )\n",
    "                            ):\n",
    "\n",
    "        mask = zone.trigger(detections=detections)\n",
    "        detections_filtered = detections[mask]\n",
    "\n",
    "        points = detections_filtered.get_anchors_coordinates(\n",
    "                anchor=sv.Position.BOTTOM_CENTER)\n",
    "\n",
    "        # plug the view transformer into an existing detection pipeline\n",
    "\n",
    "        points = view_transformer.transform_points(points=points).astype(int)\n",
    "\n",
    "        for tracker_id, [_, y] in zip(detections_filtered.tracker_id, points):\n",
    "            coordinate[tracker_id].append(y)\n",
    "\n",
    "        # wait to have enough data\n",
    "        for tracker_id in detections_filtered.tracker_id:\n",
    "                        if len(coordinate[tracker_id]) < fps/2:\n",
    "                            # print(coordinates[tracker_id], \" - id :\", tracker_id, 'len : ', len(coordinates[tracker_id]))\n",
    "                            speed_label.append(f\"#{tracker_id}\")\n",
    "\n",
    "                        else:\n",
    "                            try:\n",
    "                                coordinate_start = coordinate[tracker_id][-1]\n",
    "                                coordinate_end = coordinate[tracker_id][0]\n",
    "                                distance = abs(coordinate_start - coordinate_end)\n",
    "                                time = len(coordinate[tracker_id]) / fps\n",
    "                                speed = distance / time * 3.6\n",
    "                                speed_label.append(f\"{int(speed)} km/h\")\n",
    "\n",
    "                            except:\n",
    "\n",
    "                                speed_label.append(f\"#{tracker_id}\")\n",
    "\n",
    "                                pass\n",
    "        # labels = [\n",
    "        # f\"#{tracker_id} \"\n",
    "        # for _,_,_,_,tracker_id in detections_filtered]\n",
    "        # line_zone.trigger(detections=detections_filtered)\n",
    "        annotated_frame = sv.draw_line(scene=annotated_frame, start=line_start, end=line_end, color=colors.by_idx(i) )\n",
    "        # annotated_frame = zone_annotator.annotate(scene=annotated_frame, label=f\"Dir. Ouest : {i+random.randint(0,100)}\")\n",
    "\n",
    "        annotated_frame = zone_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            label=f\"Dir. Ouest : {line_zone.in_count}\") if i==0 else zone_annotator.annotate(\n",
    "                scene=annotated_frame, label=f\"Dir. Est : {line_zone.out_count}\")\n",
    "\n",
    "        annotated_frame = label_annotator.annotate(scene=annotated_frame,\n",
    "                                                  detections=detections_filtered,\n",
    "                                                  labels=speed_label)\n",
    "\n",
    "        # annotated_frame=line_zone_annotator.annotate(annotated_frame,line_counter=line_zone )\n",
    "        annotated_frame = box_annotator.annotate(scene=annotated_frame,\n",
    "                                                  detections=detections_filtered,\n",
    "                                                  )\n",
    "\n",
    "        annotated_frame = trace_annotator.annotate(scene=annotated_frame,detections=detections_filtered )\n",
    "        line_zone.trigger(detections=detections_filtered)\n",
    "        # print(line_zone.in_count)\n",
    "        # print(line_zone.out_count)\n",
    "\n",
    "\n",
    "    return annotated_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coordinates = [defaultdict(lambda: deque(maxlen=30)) for _ in range(3)]\\n\\ndef process_frame(frame: np.ndarray, fps: int) -> np.ndarray:\\n    # List to store speed labels for each zone\\n    speed_labels = [[] for _ in range(3)]\\n\\n    # Get detections from the model\\n    results = model(frame, imgsz=640, verbose=False)[0]\\n    detections = sv.Detections.from_ultralytics(results)\\n    detections = detections[np.isin(detections.class_id, selected_classes)]\\n\\n    # Update detections using the tracker\\n    detections = byte_tracker.update_with_detections(detections)\\n\\n    # Get the anchor points for filtered detections\\n    points = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\\n\\n    # Initialize annotated_frame with the original frame\\n    annotated_frame = frame.copy()\\n\\n    # Iterate through zones and corresponding annotators\\n    for i, (zone, zone_annotator, box_annotator, trace_annotator, line_zone,\\n            line_zone_annotator, label_annotator, line_start, line_end,\\n            view_transformer, speed_label, coordinate) in enumerate(zip(\\n            zones, zone_annotators, box_annotators, trace_annotators,\\n            line_zones, line_zone_annotators, label_annotators, lines_start,\\n            lines_end, view_transformers, speed_labels, coordinates)):\\n\\n        # Trigger zone and filter detections\\n        mask = zone.trigger(detections=detections)\\n        detections_filtered = detections[mask]\\n\\n        # Transform points and update coordinates\\n        points_transformed = view_transformer.transform_points(points=points[mask]).astype(int)\\n        for tracker_id, [_, y] in zip(detections_filtered.tracker_id, points_transformed):\\n            coordinate[tracker_id].append(y)\\n\\n        # Calculate speed for each tracked object\\n        for tracker_id in detections_filtered.tracker_id:\\n            if len(coordinate[tracker_id]) >= fps / 2:\\n                try:\\n                    distance = abs(coordinate[tracker_id][-1] - coordinate[tracker_id][0])\\n                    time = len(coordinate[tracker_id]) / fps\\n                    speed = distance / time * 3.6\\n                    speed_label.append(f\"{int(speed)} km/h\")\\n                except Exception:\\n                    speed_label.append(f\"#{tracker_id}\")\\n            else:\\n                speed_label.append(f\"#{tracker_id}\")\\n\\n        # Annotate the frame\\n        annotated_frame = sv.draw_line(annotated_frame, start=line_start, end=line_end, color=colors.by_idx(i))\\n        annotated_frame = zone_annotator.annotate(\\n            scene=annotated_frame,\\n            label=f\"Dir. Ouest : {line_zone.in_count}\" if i == 0 else f\"Dir. Est : {line_zone.out_count}\"\\n        )\\n        annotated_frame = label_annotator.annotate(annotated_frame, detections=detections_filtered, labels=speed_label)\\n        annotated_frame = box_annotator.annotate(annotated_frame, detections=detections_filtered)\\n        annotated_frame = trace_annotator.annotate(annotated_frame, detections=detections_filtered)\\n\\n        # Trigger the line zone for counting\\n        line_zone.trigger(detections=detections_filtered)\\n\\n    return annotated_frame'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''coordinates = [defaultdict(lambda: deque(maxlen=30)) for _ in range(3)]\n",
    "\n",
    "def process_frame(frame: np.ndarray, fps: int) -> np.ndarray:\n",
    "    # List to store speed labels for each zone\n",
    "    speed_labels = [[] for _ in range(3)]\n",
    "\n",
    "    # Get detections from the model\n",
    "    results = model(frame, imgsz=640, verbose=False)[0]\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    detections = detections[np.isin(detections.class_id, selected_classes)]\n",
    "\n",
    "    # Update detections using the tracker\n",
    "    detections = byte_tracker.update_with_detections(detections)\n",
    "\n",
    "    # Get the anchor points for filtered detections\n",
    "    points = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
    "\n",
    "    # Initialize annotated_frame with the original frame\n",
    "    annotated_frame = frame.copy()\n",
    "\n",
    "    # Iterate through zones and corresponding annotators\n",
    "    for i, (zone, zone_annotator, box_annotator, trace_annotator, line_zone,\n",
    "            line_zone_annotator, label_annotator, line_start, line_end,\n",
    "            view_transformer, speed_label, coordinate) in enumerate(zip(\n",
    "            zones, zone_annotators, box_annotators, trace_annotators,\n",
    "            line_zones, line_zone_annotators, label_annotators, lines_start,\n",
    "            lines_end, view_transformers, speed_labels, coordinates)):\n",
    "\n",
    "        # Trigger zone and filter detections\n",
    "        mask = zone.trigger(detections=detections)\n",
    "        detections_filtered = detections[mask]\n",
    "\n",
    "        # Transform points and update coordinates\n",
    "        points_transformed = view_transformer.transform_points(points=points[mask]).astype(int)\n",
    "        for tracker_id, [_, y] in zip(detections_filtered.tracker_id, points_transformed):\n",
    "            coordinate[tracker_id].append(y)\n",
    "\n",
    "        # Calculate speed for each tracked object\n",
    "        for tracker_id in detections_filtered.tracker_id:\n",
    "            if len(coordinate[tracker_id]) >= fps / 2:\n",
    "                try:\n",
    "                    distance = abs(coordinate[tracker_id][-1] - coordinate[tracker_id][0])\n",
    "                    time = len(coordinate[tracker_id]) / fps\n",
    "                    speed = distance / time * 3.6\n",
    "                    speed_label.append(f\"{int(speed)} km/h\")\n",
    "                except Exception:\n",
    "                    speed_label.append(f\"#{tracker_id}\")\n",
    "            else:\n",
    "                speed_label.append(f\"#{tracker_id}\")\n",
    "\n",
    "        # Annotate the frame\n",
    "        annotated_frame = sv.draw_line(annotated_frame, start=line_start, end=line_end, color=colors.by_idx(i))\n",
    "        annotated_frame = zone_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            label=f\"Dir. Ouest : {line_zone.in_count}\" if i == 0 else f\"Dir. Est : {line_zone.out_count}\"\n",
    "        )\n",
    "        annotated_frame = label_annotator.annotate(annotated_frame, detections=detections_filtered, labels=speed_label)\n",
    "        annotated_frame = box_annotator.annotate(annotated_frame, detections=detections_filtered)\n",
    "        annotated_frame = trace_annotator.annotate(annotated_frame, detections=detections_filtered)\n",
    "\n",
    "        # Trigger the line zone for counting\n",
    "        line_zone.trigger(detections=detections_filtered)\n",
    "\n",
    "    return annotated_frame'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "executionInfo": {
     "elapsed": 2171,
     "status": "error",
     "timestamp": 1722877035983,
     "user": {
      "displayName": "Ayaan Ahmed",
      "userId": "15158203945446334434"
     },
     "user_tz": -330
    },
    "id": "JJdPX_OKUFz3",
    "outputId": "ac669624-c16d-4a3b-c933-d6b5f72d6979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 30\n",
      "image : 1280x720\n"
     ]
    }
   ],
   "source": [
    "# for direct show\n",
    "cap = cv2.VideoCapture(VIDEO)\n",
    "#fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "fps=video_info.fps\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(f\"FPS: {fps}\")\n",
    "print(f\"image : {width}x{height}\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # frame=cv2.resize(frame,(1280,720))\n",
    "    show=process_frame(frame,int(fps))\n",
    "\n",
    "    fps_monitor.tick()\n",
    "    #fps = fps_monitor()\n",
    "    fps_text = f\"FPS: {fps:.0f}\"\n",
    "    cv2.putText(show, fps_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Counting\", show)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FV6nXHZ8UJAN"
   },
   "outputs": [],
   "source": [
    "# to save video instead of displaying \n",
    "side=0\n",
    "output_file = 'output_video.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  #for mp4\n",
    "out = cv2.VideoWriter(output_file, fourcc, video_info.fps, (video_info.width, video_info.height))\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(f\"FPS: {fps}\")\n",
    "print(f\"image : {width}x{height}\")\n",
    "# start time of the recording\n",
    "start_time = time.time()\n",
    "# duration of the recording\n",
    "duration = 120\n",
    "\n",
    "while (time.time() - start_time) < duration:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # frame=cv2.resize(frame,(1280,720))\n",
    "    show=process_frame(frame,0,fps)\n",
    "    fps_monitor.tick()\n",
    "    fps = fps_monitor()\n",
    "    fps_text = f\"FPS: {fps:.0f}\"\n",
    "    cv2.putText(show, fps_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # cv2.imshow(\"Counting\", show)\n",
    "    out.write(show)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vidgear.gears import CamGear\n",
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "from collections import defaultdict, deque\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    def video_manifest_extractor(source):\n",
    "        \"\"\"\n",
    "        Function to extract metadata from a YouTube video source\n",
    "          and find the desired resolution URL.\n",
    "\n",
    "        Parameters:\n",
    "        source (str): Video source URL (ex. \"<https://youtu.be/bvetuLwJIkA>\")\n",
    "\n",
    "        Returns:\n",
    "        str: Desired resolution video URL\n",
    "        \"\"\"\n",
    "        stream = CamGear(source=source, stream_mode=True, logging=True,\n",
    "                         time_delay=0).start()\n",
    "        video_metadata = stream.ytv_metadata\n",
    "\n",
    "        print(video_metadata.keys())\n",
    "        print(video_metadata[\"fps\"])\n",
    "        print(video_metadata[\"format\"])\n",
    "        print(video_metadata[\"format_index\"])\n",
    "\n",
    "        resolutions = [format[\"resolution\"] for format in video_metadata[\"formats\"]]\n",
    "        for res in resolutions:\n",
    "            print(res)\n",
    "\n",
    "        resolution_desiree = '1280x720'\n",
    "        for format in video_metadata[\"formats\"]:\n",
    "            if format[\"resolution\"] == resolution_desiree:\n",
    "                VIDEO = format[\"url\"]\n",
    "                return VIDEO\n",
    "\n",
    "    source = \"https://youtu.be/z545k7Tcb5o\"\n",
    "    VIDEO = video_manifest_extractor(source)\n",
    "    print(VIDEO)\n",
    "\n",
    "    # Load OpenVINO model \n",
    "    MODEL = \"models/yolov8s.pt\"\n",
    "    model = YOLO(MODEL)\n",
    "\n",
    "    # Get class names dictionary\n",
    "    CLASS_NAMES_DICT = model.model.names\n",
    "    print(CLASS_NAMES_DICT)\n",
    "\n",
    "    model_openvino = YOLO(\"models/yolov8s_openvino_model/\", task=\"detect\")\n",
    "    colors = sv.ColorPalette.LEGACY\n",
    "\n",
    "    video_info = sv.VideoInfo.from_video_path(VIDEO)\n",
    "\n",
    "    # Calculate the scaling coefficient based on the video width and\n",
    "    #  the desired output width (1280)\n",
    "    coef = video_info.width / 1280\n",
    "    # print(coef)\n",
    "\n",
    "    # polygon design\n",
    "    #  ----> x\n",
    "    # |         (x4,y4)   (x3,y3)\n",
    "    # |              +-------+\n",
    "    #               +-------+\n",
    "    # y            +-------+\n",
    "    #         (x1,y1)    (x2,y2)\n",
    "\n",
    "    # 3 polygons so 3 values in each coordinate from left to right \n",
    "    #    [zone1,zone2, zone3]\n",
    "    x1 = [-160, -25, 971]\n",
    "    y1 = [405, 710, 671]\n",
    "    x2 = [112, 568, 1480]\n",
    "    y2 = [503, 710, 671]\n",
    "    x3 = [557, 706, 874]\n",
    "    y3 = [195, 212, 212]\n",
    "    x4 = [411, 569, 749]\n",
    "    y4 = [195, 212, 212]\n",
    "\n",
    "    # Scale coordinates according to the video flow and the \n",
    "    # aspect ratio of the displayed video\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = map(\n",
    "        lambda vals: list(map(lambda val: val * coef, vals)),\n",
    "        [x1, y1, x2, y2, x3, y3, x4, y4]\n",
    "    )\n",
    "\n",
    "    # Find the centroid or third point from top of the polygon\n",
    "    # e.g.: ((x1 + 2* x4) / 3) for drawing counting lines\n",
    "    x14 = list(map(lambda x1, x4: (x1 + 2 * x4) / 3, x1, x4))\n",
    "    y14 = list(map(lambda y1, y4: (y1 + 2 * y4) / 3, y1, y4))\n",
    "    x23 = list(map(lambda x2, x3: (x2 + 2 * x3) / 3, x2, x3))\n",
    "    y23 = list(map(lambda y2, y3: (y2 + 2 * y3) / 3, y2, y3))\n",
    "\n",
    "    # Polygon zones defined from left to right (the same order\n",
    "    #  as the linezone)\n",
    "    polygons = [\n",
    "        np.array([[x1, y1], [x2, y2], [x3, y3], [x4, y4]], dtype=np.int32)\n",
    "        for x1, y1, x2, y2, x3, y3, x4, y4\n",
    "        in zip(x1, y1, x2, y2, x3, y3, x4, y4)\n",
    "    ]\n",
    "    # initialize our zones\n",
    "    zones = [\n",
    "        sv.PolygonZone(\n",
    "            polygon=polygon,\n",
    "            frame_resolution_wh=video_info.resolution_wh\n",
    "        )\n",
    "        for polygon\n",
    "        in polygons\n",
    "    ]\n",
    "    zone_annotators = [\n",
    "        sv.PolygonZoneAnnotator(\n",
    "            zone=zone,\n",
    "            color=colors.by_idx(index),\n",
    "            thickness=2,\n",
    "            text_thickness=1,\n",
    "            text_scale=0.5,\n",
    "        )\n",
    "        for index, zone\n",
    "        in enumerate(zones)\n",
    "    ]\n",
    "    label_annotators = [\n",
    "        sv.LabelAnnotator(\n",
    "            text_position=sv.Position.TOP_CENTER,\n",
    "            color=colors.by_idx(index),\n",
    "            text_thickness=1,\n",
    "            text_scale=0.5,\n",
    "        )\n",
    "        for index\n",
    "        in range(len(zones))\n",
    "    ]\n",
    "    # box_annotators = [\n",
    "    #     sv.BoxAnnotator(\n",
    "    #         color=colors.by_idx(index),\n",
    "    #         thickness=1,\n",
    "    #         text_thickness=1,\n",
    "    #         text_scale=0.5\n",
    "    #         )\n",
    "    #     for index\n",
    "    #     in range(len(polygons))\n",
    "    # ]\n",
    "    box_annotators = [\n",
    "        sv.BoundingBoxAnnotator(\n",
    "            color=colors.by_idx(index),\n",
    "            thickness=1,\n",
    "            )\n",
    "        for index\n",
    "        in range(len(polygons))\n",
    "    ]\n",
    "\n",
    "    trace_annotators = [\n",
    "        sv.TraceAnnotator(\n",
    "            color=colors.by_idx(index),\n",
    "            thickness=1,\n",
    "            trace_length=video_info.fps * 1.5,\n",
    "            position=sv.Position.BOTTOM_CENTER,\n",
    "            )\n",
    "        for index\n",
    "        in range(len(polygons))\n",
    "    ]\n",
    "    lines_start = [\n",
    "        sv.Point(x14, y14)\n",
    "        for x14, y14\n",
    "        in zip(x14, y14)\n",
    "    ]\n",
    "    lines_end = [\n",
    "        sv.Point(x23, y23)\n",
    "        for x23, y23\n",
    "        in zip(x23, y23)\n",
    "    ]\n",
    "    positions = [\n",
    "        (sv.Position.CENTER, sv.Position.CENTER),\n",
    "        (sv.Position.CENTER, sv.Position.CENTER),\n",
    "        (sv.Position.CENTER, sv.Position.CENTER),\n",
    "    ]\n",
    "    line_zones = [\n",
    "        sv.LineZone(start=line_start, end=line_end,\n",
    "                    triggering_anchors=position)\n",
    "        for line_start, line_end, position\n",
    "        in zip(lines_start, lines_end, positions)\n",
    "    ]\n",
    "    # for automatic line zone annotator not use here want to use a custom one\n",
    "    line_zone_annotators = [\n",
    "        sv.LineZoneAnnotator(thickness=1,\n",
    "                             color=colors.by_idx(index),\n",
    "                             text_thickness=1,\n",
    "                             text_scale=0.5,\n",
    "                             text_offset=4)\n",
    "        for index\n",
    "        in range(len(line_zones))\n",
    "    ]\n",
    "    # couting line zone text position\n",
    "    # text_pos = [\n",
    "    #     sv.Point(x=100, y=320),\n",
    "    #     sv.Point(x=700, y=320),\n",
    "    #     sv.Point(x=1077, y=320)\n",
    "    # ]\n",
    "    # initialyze ByteTracker\n",
    "    byte_tracker = sv.ByteTrack(\n",
    "        track_thresh=0.25,\n",
    "        track_buffer=100, \n",
    "        match_thresh=0.8,\n",
    "        frame_rate=video_info.fps\n",
    "    )\n",
    "    # byte_tracker = sv.ByteTrack()\n",
    "    fps_monitor = sv.FPSMonitor()\n",
    "    # heat_map = sv.HeatMapAnnotator()\n",
    "    # smoother = sv.DetectionsSmoother()\n",
    "    # intialize the source coordinate for speed estimation\n",
    "    SOURCES = np.array([[\n",
    "        [x4[0], y4[0]],\n",
    "        [x3[0], y3[0]],\n",
    "        [x2[0], y2[0]],\n",
    "        [x1[0], y1[0]]\n",
    "\n",
    "    ], [[x4[1], y4[1]],\n",
    "        [x3[1], y3[1]],\n",
    "        [x2[1], y2[1]],\n",
    "        [x1[1], y1[1]]],\n",
    "          [[x4[2], y4[2]],\n",
    "           [x3[2], y3[2]],\n",
    "           [x2[2], y2[2]],\n",
    "           [x1[2], y1[2]]\n",
    "           ]\n",
    "           ])\n",
    "    # initialize Target real(in meters) coordinate \n",
    "    # zone1 in meters\n",
    "    TARGET_WIDTH = 6\n",
    "    TARGET_HEIGHT = 75\n",
    "    TARGETS = np.array([\n",
    "        [0, 0],\n",
    "        [TARGET_WIDTH - 1, 0],\n",
    "        [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "        [0, TARGET_HEIGHT - 1],\n",
    "    ])\n",
    "    # zone 2 in meters\n",
    "    TARGET_WIDTH = 6\n",
    "    TARGET_HEIGHT = 85\n",
    "    TARGETS = np.append(TARGETS, np.array([\n",
    "        [0, 0],\n",
    "        [TARGET_WIDTH - 1, 0],\n",
    "        [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "        [0, TARGET_HEIGHT - 1],\n",
    "    ]), axis=0)\n",
    "    # zone3 in meters\n",
    "    TARGET_WIDTH = 6\n",
    "    TARGET_HEIGHT = 80\n",
    "    TARGETS = np.append(TARGETS, np.array([\n",
    "        [0, 0],\n",
    "        [TARGET_WIDTH - 1, 0],\n",
    "        [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "        [0, TARGET_HEIGHT - 1],\n",
    "    ]), axis=0)\n",
    "\n",
    "    TARGETS = TARGETS.reshape(3, 4, 2)\n",
    "    # class searching transformation matrix between\n",
    "    #  SOURCE and TARGET to get speed estimation\n",
    "\n",
    "    class ViewTransformer:\n",
    "\n",
    "        def __init__(self, source: np.ndarray, target: np.ndarray) -> None:\n",
    "            source = source.astype(np.float32)\n",
    "            target = target.astype(np.float32)\n",
    "            self.m = cv2.getPerspectiveTransform(source, target)\n",
    "\n",
    "        def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
    "            if points.size == 0:\n",
    "                return points\n",
    "\n",
    "            reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
    "            transformed_points = cv2.perspectiveTransform(\n",
    "                    reshaped_points, self.m)\n",
    "            return transformed_points.reshape(-1, 2)\n",
    "\n",
    "    # create the transformers matrix for each zone\n",
    "    view_transformers = [\n",
    "        ViewTransformer(source=s, target=t)\n",
    "        for s, t\n",
    "        in zip(SOURCES, TARGETS)\n",
    "    ]\n",
    "    # car, motorcycle, bus, truck from coco classes\n",
    "    selected_classes = [2, 3, 5, 7] \n",
    "    # initialize the dictionary that we \n",
    "    # will use to store the coordinates for each zone\n",
    "    coordinates = defaultdict(lambda: deque(maxlen=30))\n",
    "    coordinates = defaultdict(lambda: deque(maxlen=30))\n",
    "    coordinates = np.append(coordinates, defaultdict(lambda: deque(maxlen=30)))\n",
    "    coordinates = np.append(coordinates, defaultdict(lambda: deque(maxlen=30)))\n",
    "    # frame processing\n",
    "\n",
    "    '''def process_frame(frame: np.ndarray, fps) -> np.ndarray:\n",
    "        speed_labels = [], [], []\n",
    "        zone_car_counts = [0]*len(zones)  # Initialize car counts for each zone\n",
    "        warning_message = \"\"\n",
    "\n",
    "        results = model_openvino(frame, imgsz=640, verbose=False)[0]\n",
    "        detections = sv.Detections.from_ultralytics(results)\n",
    "        detections = detections[np.isin(detections.class_id, selected_classes)]  # Filter on selected classes\n",
    "        detections = byte_tracker.update_with_detections(detections)\n",
    "\n",
    "    # Copy frame before annotating                     \n",
    "        annotated_frame = frame.copy()\n",
    "\n",
    "        for i, (zone,\n",
    "            zone_annotator,\n",
    "            box_annotator,\n",
    "            trace_annotator,\n",
    "            line_zone,\n",
    "            line_zone_annotator,\n",
    "            label_annotator,\n",
    "            line_start,\n",
    "            line_end,\n",
    "            view_transformer,\n",
    "            speed_label,\n",
    "            coordinate) in enumerate(zip(\n",
    "                zones,\n",
    "                zone_annotators,\n",
    "                box_annotators,\n",
    "                trace_annotators,\n",
    "                line_zones,\n",
    "                line_zone_annotators,\n",
    "                label_annotators,\n",
    "                lines_start,\n",
    "                lines_end,\n",
    "                view_transformers,\n",
    "                speed_labels,\n",
    "                coordinates)):\n",
    "\n",
    "            mask = zone.trigger(detections=detections)\n",
    "            detections_filtered = detections[mask]\n",
    "            zone_car_counts[i] = len(detections_filtered)  # Count cars in this zone\n",
    "\n",
    "            points = detections_filtered.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
    "            points = view_transformer.transform_points(points=points).astype(int)\n",
    "\n",
    "        # Calculate distances between cars and check for closeness\n",
    "            for j, (point1, tracker_id1) in enumerate(zip(points, detections_filtered.tracker_id)):\n",
    "                for k, (point2, tracker_id2) in enumerate(zip(points, detections_filtered.tracker_id)):\n",
    "                    if j != k:\n",
    "                        distance = np.linalg.norm(point1 - point2)\n",
    "                        if distance < 50:  # Threshold for being \"too close\"\n",
    "                            warning_message += f\"Cars {tracker_id1} and {tracker_id2} too close in zone {i+1}: {distance:.2f} pixels\\n\"\n",
    "\n",
    "            for tracker_id, [_, y] in zip(detections_filtered.tracker_id, points):\n",
    "                coordinate[tracker_id].append(y)\n",
    "\n",
    "        # Calculate speed and generate labels\n",
    "            for tracker_id in detections_filtered.tracker_id:\n",
    "                if len(coordinate[tracker_id]) < fps/2:\n",
    "                    speed_label.append(f\"#{tracker_id}\")\n",
    "                else:\n",
    "                    try:\n",
    "                        coordinate_start = coordinate[tracker_id][-1]\n",
    "                        coordinate_end = coordinate[tracker_id][0]\n",
    "                        distance = abs(coordinate_start - coordinate_end)\n",
    "                        time = len(coordinate[tracker_id]) / fps\n",
    "                        speed = distance / time * 3.6\n",
    "                        speed_label.append(f\"{int(speed)} km/h\")\n",
    "                    except Exception as e:\n",
    "                        speed_label.append(f\"#{tracker_id}\")\n",
    "                        print(f\"An error occurred: {e}\")\n",
    "\n",
    "        # Annotate frame\n",
    "            direction_label = \"Dir. West\" if i == 0 else \"Dir. East\"\n",
    "            annotated_frame = zone_annotator.annotate(\n",
    "                scene=annotated_frame,\n",
    "                label=f\"{direction_label} : {line_zone.in_count}\") if i == 0 else zone_annotator.annotate(\n",
    "                scene=annotated_frame,\n",
    "                label=f\"{direction_label} : {line_zone.out_count}\")\n",
    "\n",
    "            annotated_frame = label_annotator.annotate(\n",
    "                scene=annotated_frame,\n",
    "                detections=detections_filtered,\n",
    "                labels=speed_label)\n",
    "\n",
    "            annotated_frame = box_annotator.annotate(\n",
    "                scene=annotated_frame,\n",
    "                detections=detections_filtered)\n",
    "\n",
    "            annotated_frame = trace_annotator.annotate(\n",
    "                scene=annotated_frame,\n",
    "                detections=detections_filtered)\n",
    "\n",
    "            annotated_frame = sv.draw_line(scene=annotated_frame,\n",
    "                                           start=line_start,\n",
    "                                       end=line_end,\n",
    "                                       color=colors.by_idx(i))\n",
    "\n",
    "            line_zone.trigger(detections=detections_filtered)\n",
    "\n",
    "    # Display total car count and warnings on the top of the screen\n",
    "        total_car_count = sum(zone_car_counts)\n",
    "        cv2.putText(annotated_frame, f\"Total Cars: {total_car_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    \n",
    "        if warning_message:\n",
    "            cv2.putText(annotated_frame, \"WARNING: Cars too close!\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            print(warning_message)  # Also print the warning message in the backend\n",
    "\n",
    "        return annotated_frame'''\n",
    "    def process_frame(frame: np.ndarray, fps) -> np.ndarray:\n",
    "        speed_labels = [], [], []\n",
    "        zone_car_counts = [0] * len(zones)  # Initialize car counts for each zone\n",
    "        warning_message = \"\"\n",
    "\n",
    "        results = model_openvino(frame, imgsz=640, verbose=False)[0]\n",
    "        detections = sv.Detections.from_ultralytics(results)\n",
    "        detections = detections[np.isin(detections.class_id, selected_classes)]  # Filter on selected classes\n",
    "        detections = byte_tracker.update_with_detections(detections)\n",
    "\n",
    "    # Copy frame before annotating                     \n",
    "        annotated_frame = frame.copy()\n",
    "\n",
    "        for i, (zone,\n",
    "            zone_annotator,\n",
    "            box_annotator,\n",
    "            trace_annotator,\n",
    "            line_zone,\n",
    "            line_zone_annotator,\n",
    "            label_annotator,\n",
    "            line_start,\n",
    "            line_end,\n",
    "            view_transformer,\n",
    "            speed_label,\n",
    "            coordinate) in enumerate(zip(\n",
    "                zones,\n",
    "                zone_annotators,\n",
    "                box_annotators,\n",
    "                trace_annotators,\n",
    "                line_zones,\n",
    "                line_zone_annotators,\n",
    "                label_annotators,\n",
    "                lines_start,\n",
    "                lines_end,\n",
    "                view_transformers,\n",
    "                speed_labels,\n",
    "                coordinates)):\n",
    "\n",
    "            mask = zone.trigger(detections=detections)\n",
    "            detections_filtered = detections[mask]\n",
    "            zone_car_counts[i] = len(detections_filtered)  # Count cars in this zone\n",
    "\n",
    "            points = detections_filtered.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
    "            points = view_transformer.transform_points(points=points).astype(int)\n",
    "\n",
    "        # Calculate distances between cars and check for closeness\n",
    "            for j, (point1, tracker_id1) in enumerate(zip(points, detections_filtered.tracker_id)):\n",
    "                for k, (point2, tracker_id2) in enumerate(zip(points, detections_filtered.tracker_id)):\n",
    "                    if j != k:\n",
    "                        distance = np.linalg.norm(point1 - point2)\n",
    "                        if distance < 50:  # Threshold for being \"too close\"\n",
    "                            warning_message += f\"Cars {tracker_id1} and {tracker_id2} too close in zone {i+1}: {distance:.2f} pixels\\n\"\n",
    "\n",
    "            for tracker_id, [_, y] in zip(detections_filtered.tracker_id, points):\n",
    "                coordinate[tracker_id].append(y)\n",
    "\n",
    "        # Calculate speed and generate labels\n",
    "            for tracker_id in detections_filtered.tracker_id:\n",
    "                if len(coordinate[tracker_id]) < fps/2:\n",
    "                    speed_label.append(f\"#{tracker_id}\")\n",
    "                else:\n",
    "                    try:\n",
    "                        coordinate_start = coordinate[tracker_id][-1]\n",
    "                        coordinate_end = coordinate[tracker_id][0]\n",
    "                        distance = abs(coordinate_start - coordinate_end)\n",
    "                        time = len(coordinate[tracker_id]) / fps\n",
    "                        speed = distance / time * 3.6\n",
    "                        speed_label.append(f\"{int(speed)} km/h\")\n",
    "                    except Exception as e:\n",
    "                        speed_label.append(f\"#{tracker_id}\")\n",
    "                        print(f\"An error occurred: {e}\")\n",
    "\n",
    "        # Annotate frame\n",
    "            direction_label = \"Dir. West\" if i == 0 else \"Dir. East\"\n",
    "            annotated_frame = zone_annotator.annotate(\n",
    "                scene=annotated_frame,\n",
    "                label=f\"{direction_label} : {line_zone.in_count}\") if i == 0 else zone_annotator.annotate(\n",
    "                scene=annotated_frame,\n",
    "                label=f\"{direction_label} : {line_zone.out_count}\")\n",
    "\n",
    "            annotated_frame = label_annotator.annotate(\n",
    "                scene=annotated_frame,\n",
    "                detections=detections_filtered,\n",
    "                labels=speed_label)\n",
    "\n",
    "            annotated_frame = box_annotator.annotate(\n",
    "                scene=annotated_frame,\n",
    "                detections=detections_filtered)\n",
    "\n",
    "            annotated_frame = trace_annotator.annotate(\n",
    "                scene=annotated_frame,\n",
    "                detections=detections_filtered)\n",
    "\n",
    "            annotated_frame = sv.draw_line(scene=annotated_frame,\n",
    "                                       start=line_start,\n",
    "                                       end=line_end,\n",
    "                                       color=colors.by_idx(i))\n",
    "\n",
    "            line_zone.trigger(detections=detections_filtered)\n",
    "\n",
    "    # Display total car count and warnings in the center of the screen\n",
    "        total_car_count = sum(zone_car_counts)\n",
    "        height, width, _ = annotated_frame.shape\n",
    "        text_position = (width - 300, 50)\n",
    "        warning_text_position = (width - 500, 100)\n",
    "    \n",
    "        cv2.putText(annotated_frame, f\"Total Cars: {total_car_count}\", text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    \n",
    "        if warning_message:\n",
    "            cv2.putText(annotated_frame, \"WARNING: Cars too close!\", warning_text_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            print(warning_message)  # Also print the warning message in the backend\n",
    "\n",
    "        return annotated_frame\n",
    "\n",
    "\n",
    "    # for direct show\n",
    "    cap = cv2.VideoCapture(VIDEO)\n",
    "    # fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    fps = video_info.fps\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    print(f\"FPS: {fps}\")\n",
    "    print(f\"image : {width}x{height}\")\n",
    "\n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # frame=cv2.resize(frame,(1280,720))\n",
    "        show = process_frame(frame, int(fps))\n",
    "        fps_monitor.tick()\n",
    "        #fps = fps_monitor()\n",
    "        fps_text = f\"FPS: {fps:.0f}\"\n",
    "        cv2.putText(show, fps_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Counting - Speed Estimation\", show)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "        time.sleep(max(1/25 - (time.time() - start_time), 0))\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***MORE FEATURES***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vidgear.gears import CamGear\n",
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "from collections import defaultdict, deque\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    def video_manifest_extractor(source):\n",
    "        \"\"\"\n",
    "        Function to extract metadata from a YouTube video source\n",
    "          and find the desired resolution URL.\n",
    "\n",
    "        Parameters:\n",
    "        source (str): Video source URL (ex. \"<https://youtu.be/bvetuLwJIkA>\")\n",
    "\n",
    "        Returns:\n",
    "        str: Desired resolution video URL\n",
    "        \"\"\"\n",
    "        stream = CamGear(source=source, stream_mode=True, logging=True,\n",
    "                         time_delay=0).start()\n",
    "        video_metadata = stream.ytv_metadata\n",
    "\n",
    "        print(video_metadata.keys())\n",
    "        print(video_metadata[\"fps\"])\n",
    "        print(video_metadata[\"format\"])\n",
    "        print(video_metadata[\"format_index\"])\n",
    "\n",
    "        resolutions = [format[\"resolution\"] for format in video_metadata[\"formats\"]]\n",
    "        for res in resolutions:\n",
    "            print(res)\n",
    "\n",
    "        resolution_desiree = '1280x720'\n",
    "        for format in video_metadata[\"formats\"]:\n",
    "            if format[\"resolution\"] == resolution_desiree:\n",
    "                VIDEO = format[\"url\"]\n",
    "                return VIDEO\n",
    "\n",
    "    source = \"https://youtu.be/z545k7Tcb5o\"\n",
    "    VIDEO = video_manifest_extractor(source)\n",
    "    print(VIDEO)\n",
    "\n",
    "    # Load OpenVINO model \n",
    "    MODEL = \"models/yolov8s.pt\"\n",
    "    model = YOLO(MODEL)\n",
    "\n",
    "    # Get class names dictionary\n",
    "    CLASS_NAMES_DICT = model.model.names\n",
    "    print(CLASS_NAMES_DICT)\n",
    "\n",
    "    model_openvino = YOLO(\"models/yolov8s_openvino_model/\", task=\"detect\")\n",
    "    colors = sv.ColorPalette.LEGACY\n",
    "\n",
    "    video_info = sv.VideoInfo.from_video_path(VIDEO)\n",
    "\n",
    "    # Calculate the scaling coefficient based on the video width and\n",
    "    #  the desired output width (1280)\n",
    "    coef = video_info.width / 1280\n",
    "    # print(coef)\n",
    "\n",
    "    # polygon design\n",
    "    #  ----> x\n",
    "    # |         (x4,y4)   (x3,y3)\n",
    "    # |              +-------+\n",
    "    #               +-------+\n",
    "    # y            +-------+\n",
    "    #         (x1,y1)    (x2,y2)\n",
    "\n",
    "    # 3 polygons so 3 values in each coordinate from left to right \n",
    "    #    [zone1,zone2, zone3]\n",
    "    x1 = [-160, -25, 971]\n",
    "    y1 = [405, 710, 671]\n",
    "    x2 = [112, 568, 1480]\n",
    "    y2 = [503, 710, 671]\n",
    "    x3 = [557, 706, 874]\n",
    "    y3 = [195, 212, 212]\n",
    "    x4 = [411, 569, 749]\n",
    "    y4 = [195, 212, 212]\n",
    "\n",
    "    # Scale coordinates according to the video flow and the \n",
    "    # aspect ratio of the displayed video\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = map(\n",
    "        lambda vals: list(map(lambda val: val * coef, vals)),\n",
    "        [x1, y1, x2, y2, x3, y3, x4, y4]\n",
    "    )\n",
    "\n",
    "    # Find the centroid or third point from top of the polygon\n",
    "    # e.g.: ((x1 + 2* x4) / 3) for drawing counting lines\n",
    "    x14 = list(map(lambda x1, x4: (x1 + 2 * x4) / 3, x1, x4))\n",
    "    y14 = list(map(lambda y1, y4: (y1 + 2 * y4) / 3, y1, y4))\n",
    "    x23 = list(map(lambda x2, x3: (x2 + 2 * x3) / 3, x2, x3))\n",
    "    y23 = list(map(lambda y2, y3: (y2 + 2 * y3) / 3, y2, y3))\n",
    "\n",
    "    # Polygon zones defined from left to right (the same order\n",
    "    #  as the linezone)\n",
    "    polygons = [\n",
    "        np.array([[x1, y1], [x2, y2], [x3, y3], [x4, y4]], dtype=np.int32)\n",
    "        for x1, y1, x2, y2, x3, y3, x4, y4\n",
    "        in zip(x1, y1, x2, y2, x3, y3, x4, y4)\n",
    "    ]\n",
    "    # initialize our zones\n",
    "    zones = [\n",
    "        sv.PolygonZone(\n",
    "            polygon=polygon,\n",
    "            frame_resolution_wh=video_info.resolution_wh\n",
    "        )\n",
    "        for polygon\n",
    "        in polygons\n",
    "    ]\n",
    "    zone_annotators = [\n",
    "        sv.PolygonZoneAnnotator(\n",
    "            zone=zone,\n",
    "            color=colors.by_idx(index),\n",
    "            thickness=2,\n",
    "            text_thickness=1,\n",
    "            text_scale=0.5,\n",
    "        )\n",
    "        for index, zone\n",
    "        in enumerate(zones)\n",
    "    ]\n",
    "    label_annotators = [\n",
    "        sv.LabelAnnotator(\n",
    "            text_position=sv.Position.TOP_CENTER,\n",
    "            color=colors.by_idx(index),\n",
    "            text_thickness=1,\n",
    "            text_scale=0.5,\n",
    "        )\n",
    "        for index\n",
    "        in range(len(zones))\n",
    "    ]\n",
    "    # box_annotators = [\n",
    "    #     sv.BoxAnnotator(\n",
    "    #         color=colors.by_idx(index),\n",
    "    #         thickness=1,\n",
    "    #         text_thickness=1,\n",
    "    #         text_scale=0.5\n",
    "    #         )\n",
    "    #     for index\n",
    "    #     in range(len(polygons))\n",
    "    # ]\n",
    "    box_annotators = [\n",
    "        sv.BoundingBoxAnnotator(\n",
    "            color=colors.by_idx(index),\n",
    "            thickness=1,\n",
    "            )\n",
    "        for index\n",
    "        in range(len(polygons))\n",
    "    ]\n",
    "\n",
    "    trace_annotators = [\n",
    "        sv.TraceAnnotator(\n",
    "            color=colors.by_idx(index),\n",
    "            thickness=1,\n",
    "            trace_length=video_info.fps * 1.5,\n",
    "            position=sv.Position.BOTTOM_CENTER,\n",
    "            )\n",
    "        for index\n",
    "        in range(len(polygons))\n",
    "    ]\n",
    "    lines_start = [\n",
    "        sv.Point(x14, y14)\n",
    "        for x14, y14\n",
    "        in zip(x14, y14)\n",
    "    ]\n",
    "    lines_end = [\n",
    "        sv.Point(x23, y23)\n",
    "        for x23, y23\n",
    "        in zip(x23, y23)\n",
    "    ]\n",
    "    positions = [\n",
    "        (sv.Position.CENTER, sv.Position.CENTER),\n",
    "        (sv.Position.CENTER, sv.Position.CENTER),\n",
    "        (sv.Position.CENTER, sv.Position.CENTER),\n",
    "    ]\n",
    "    line_zones = [\n",
    "        sv.LineZone(start=line_start, end=line_end,\n",
    "                    triggering_anchors=position)\n",
    "        for line_start, line_end, position\n",
    "        in zip(lines_start, lines_end, positions)\n",
    "    ]\n",
    "    # for automatic line zone annotator not use here want to use a custom one\n",
    "    line_zone_annotators = [\n",
    "        sv.LineZoneAnnotator(thickness=1,\n",
    "                             color=colors.by_idx(index),\n",
    "                             text_thickness=1,\n",
    "                             text_scale=0.5,\n",
    "                             text_offset=4)\n",
    "        for index\n",
    "        in range(len(line_zones))\n",
    "    ]\n",
    "    # couting line zone text position\n",
    "    # text_pos = [\n",
    "    #     sv.Point(x=100, y=320),\n",
    "    #     sv.Point(x=700, y=320),\n",
    "    #     sv.Point(x=1077, y=320)\n",
    "    # ]\n",
    "    # initialyze ByteTracker\n",
    "    byte_tracker = sv.ByteTrack(\n",
    "        track_thresh=0.25,\n",
    "        track_buffer=100, \n",
    "        match_thresh=0.8,\n",
    "        frame_rate=video_info.fps\n",
    "    )\n",
    "    # byte_tracker = sv.ByteTrack()\n",
    "    fps_monitor = sv.FPSMonitor()\n",
    "    # heat_map = sv.HeatMapAnnotator()\n",
    "    # smoother = sv.DetectionsSmoother()\n",
    "    # intialize the source coordinate for speed estimation\n",
    "    SOURCES = np.array([[\n",
    "        [x4[0], y4[0]],\n",
    "        [x3[0], y3[0]],\n",
    "        [x2[0], y2[0]],\n",
    "        [x1[0], y1[0]]\n",
    "\n",
    "    ], [[x4[1], y4[1]],\n",
    "        [x3[1], y3[1]],\n",
    "        [x2[1], y2[1]],\n",
    "        [x1[1], y1[1]]],\n",
    "          [[x4[2], y4[2]],\n",
    "           [x3[2], y3[2]],\n",
    "           [x2[2], y2[2]],\n",
    "           [x1[2], y1[2]]\n",
    "           ]\n",
    "           ])\n",
    "    # initialize Target real(in meters) coordinate \n",
    "    # zone1 in meters\n",
    "    TARGET_WIDTH = 6\n",
    "    TARGET_HEIGHT = 75\n",
    "    TARGETS = np.array([\n",
    "        [0, 0],\n",
    "        [TARGET_WIDTH - 1, 0],\n",
    "        [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "        [0, TARGET_HEIGHT - 1],\n",
    "    ])\n",
    "    # zone 2 in meters\n",
    "    TARGET_WIDTH = 6\n",
    "    TARGET_HEIGHT = 85\n",
    "    TARGETS = np.append(TARGETS, np.array([\n",
    "        [0, 0],\n",
    "        [TARGET_WIDTH - 1, 0],\n",
    "        [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "        [0, TARGET_HEIGHT - 1],\n",
    "    ]), axis=0)\n",
    "    # zone3 in meters\n",
    "    TARGET_WIDTH = 6\n",
    "    TARGET_HEIGHT = 80\n",
    "    TARGETS = np.append(TARGETS, np.array([\n",
    "        [0, 0],\n",
    "        [TARGET_WIDTH - 1, 0],\n",
    "        [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "        [0, TARGET_HEIGHT - 1],\n",
    "    ]), axis=0)\n",
    "\n",
    "    TARGETS = TARGETS.reshape(3, 4, 2)\n",
    "    # class searching transformation matrix between\n",
    "    #  SOURCE and TARGET to get speed estimation\n",
    "\n",
    "    class ViewTransformer:\n",
    "\n",
    "        def __init__(self, source: np.ndarray, target: np.ndarray) -> None:\n",
    "            source = source.astype(np.float32)\n",
    "            target = target.astype(np.float32)\n",
    "            self.m = cv2.getPerspectiveTransform(source, target)\n",
    "\n",
    "        def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
    "            if points.size == 0:\n",
    "                return points\n",
    "\n",
    "            reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
    "            transformed_points = cv2.perspectiveTransform(\n",
    "                    reshaped_points, self.m)\n",
    "            return transformed_points.reshape(-1, 2)\n",
    "\n",
    "    # create the transformers matrix for each zone\n",
    "    view_transformers = [\n",
    "        ViewTransformer(source=s, target=t)\n",
    "        for s, t\n",
    "        in zip(SOURCES, TARGETS)\n",
    "    ]\n",
    "    # car, motorcycle, bus, truck from coco classes\n",
    "    selected_classes = [2, 3, 5, 7] \n",
    "    # initialize the dictionary that we \n",
    "    # will use to store the coordinates for each zone\n",
    "    coordinates = defaultdict(lambda: deque(maxlen=30))\n",
    "    coordinates = defaultdict(lambda: deque(maxlen=30))\n",
    "    coordinates = np.append(coordinates, defaultdict(lambda: deque(maxlen=30)))\n",
    "    coordinates = np.append(coordinates, defaultdict(lambda: deque(maxlen=30)))\n",
    "    # frame processing\n",
    "    def process_frame(frame: np.ndarray, fps) -> np.ndarray:\n",
    "        speed_labels = [], [], []\n",
    "        zone_car_counts = [0] * len(zones)  # Initialize car counts for each zone based on the number of zones\n",
    "        warning_message = \"\"\n",
    "        \n",
    "        results = model_openvino(frame, imgsz=640, verbose=False)[0]\n",
    "        detections = sv.Detections.from_ultralytics(results)\n",
    "        detections = detections[np.isin(detections.class_id, selected_classes)]  # Filter on selected classes\n",
    "        detections = byte_tracker.update_with_detections(detections)\n",
    "    \n",
    "        # Copy frame before annotating                     \n",
    "        annotated_frame = frame.copy()\n",
    "        height, width, _ = annotated_frame.shape  # Get frame dimensions\n",
    "    \n",
    "        # Initialize vehicle counts by type\n",
    "        vehicle_type_counts = {class_id: 0 for class_id in selected_classes}\n",
    "    \n",
    "        for i, (zone,\n",
    "                zone_annotator,\n",
    "                box_annotator,\n",
    "                trace_annotator,\n",
    "                line_zone,\n",
    "                line_zone_annotator,\n",
    "                label_annotator,\n",
    "                line_start,\n",
    "                line_end,\n",
    "                view_transformer,\n",
    "                speed_label,\n",
    "                coordinate) in enumerate(zip(\n",
    "                    zones,\n",
    "                    zone_annotators,\n",
    "                    box_annotators,\n",
    "                    trace_annotators,\n",
    "                    line_zones,\n",
    "                    line_zone_annotators,\n",
    "                    label_annotators,\n",
    "                    lines_start,\n",
    "                    lines_end,\n",
    "                    view_transformers,\n",
    "                    speed_labels,\n",
    "                    coordinates)):\n",
    "    \n",
    "            mask = zone.trigger(detections=detections)\n",
    "            detections_filtered = detections[mask]\n",
    "            zone_car_counts[i] = len(detections_filtered)  # Count cars in this zone\n",
    "    \n",
    "            points = detections_filtered.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)\n",
    "            points = view_transformer.transform_points(points=points).astype(int)\n",
    "    \n",
    "            # Calculate distances between cars \n",
    "            for j, (point1, tracker_id1) in enumerate(zip(points, detections_filtered.tracker_id)):\n",
    "                for k, (point2, tracker_id2) in enumerate(zip(points, detections_filtered.tracker_id)):\n",
    "                    if j != k:\n",
    "                        distance = np.linalg.norm(point1 - point2)\n",
    "                        if distance < 50:  # Threshold for being \"too close\"\n",
    "                            warning_message += f\"Cars {tracker_id1} and {tracker_id2} too close in zone {i+1}: {distance:.2f} pixels\\n\"\n",
    "    \n",
    "            for tracker_id, [_, y] in zip(detections_filtered.tracker_id, points):\n",
    "                coordinate[tracker_id].append(y)\n",
    "    \n",
    "            # Calculate speed and generate labels\n",
    "            for tracker_id in detections_filtered.tracker_id:\n",
    "                if len(coordinate[tracker_id]) < fps/2:\n",
    "                    speed_label.append(f\"#{tracker_id}\")\n",
    "                else:\n",
    "                    try:\n",
    "                        coordinate_start = coordinate[tracker_id][-1]\n",
    "                        coordinate_end = coordinate[tracker_id][0]\n",
    "                        distance = abs(coordinate_start - coordinate_end)\n",
    "                        time = len(coordinate[tracker_id]) / fps\n",
    "                        speed = distance / time * 3.6\n",
    "                        speed_label.append(f\"{int(speed)} km/h\")\n",
    "    \n",
    "                        # Real-time speed monitoring with alerts\n",
    "                        if speed > speed_limit:  \n",
    "                            warning_message += f\"Car {tracker_id} in zone {i+1} is overspeeding at {int(speed)} km/h\\n\"\n",
    "    \n",
    "                    except Exception as e:\n",
    "                        speed_label.append(f\"#{tracker_id}\")\n",
    "                        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "            #Update vehicle counts by type\n",
    "            for class_id in detections_filtered.class_id:\n",
    "                vehicle_type_counts[class_id] += 1\n",
    "    \n",
    "            # Annotate frame\n",
    "            direction_label = \"Dir. West\" if i == 0 else \"Dir. East\"\n",
    "            annotated_frame = zone_annotator.annotate(\n",
    "                scene=annotated_frame,\n",
    "                label=f\"{direction_label} : {line_zone.in_count}\") if i == 0 else zone_annotator.annotate(\n",
    "                    scene=annotated_frame,\n",
    "                    label=f\"{direction_label} : {line_zone.out_count}\")\n",
    "    \n",
    "            annotated_frame = label_annotator.annotate(\n",
    "                scene=annotated_frame,\n",
    "                detections=detections_filtered,\n",
    "                labels=speed_label)\n",
    "    \n",
    "            annotated_frame = box_annotator.annotate(\n",
    "                scene=annotated_frame,\n",
    "                detections=detections_filtered)\n",
    "    \n",
    "            annotated_frame = trace_annotator.annotate(\n",
    "                scene=annotated_frame,\n",
    "                detections=detections_filtered)\n",
    "    \n",
    "            annotated_frame = sv.draw_line(scene=annotated_frame,\n",
    "                                           start=line_start,\n",
    "                                           end=line_end,\n",
    "                                           color=colors.by_idx(i))\n",
    "    \n",
    "            line_zone.trigger(detections=detections_filtered)\n",
    "    \n",
    "        # Display vehicle counts by type \n",
    "        type_text_position = (width - 300, 30)\n",
    "        vehicle_count_text = \" | \".join([f\"{class_names[class_id]}: {count}\" for class_id, count in vehicle_type_counts.items()])\n",
    "        cv2.putText(annotated_frame, f\"Vehicle Types: {vehicle_count_text}\", type_text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "    \n",
    "        # Traffic congestion detection\n",
    "        congestion_threshold = 10  # Example threshold for congestion\n",
    "        if total_car_count > congestion_threshold:\n",
    "            congestion_text_position = (width - 300, 60)\n",
    "            cv2.putText(annotated_frame, \"WARNING: Traffic Congestion Detected!\", congestion_text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "            print(f\"Traffic congestion detected: {total_car_count} vehicles.\")\n",
    "    \n",
    "        # zone-based vehicle heatmap (Basic version)\n",
    "        heatmap_color = (0, 0, 255) if total_car_count > congestion_threshold else (0, 255, 0)\n",
    "        for i, count in enumerate(zone_car_counts):\n",
    "            zone_text_position = (width - 300, 90 + i * 30)\n",
    "            cv2.putText(annotated_frame, f\"Zone {i+1} Cars: {count}\", zone_text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.6, heatmap_color, 2)\n",
    "    \n",
    "        # Display total car count and warnings \n",
    "        total_text_position = (width - 300, 120 + len(zone_car_counts) * 30)\n",
    "        cv2.putText(annotated_frame, f\"Total Cars: {total_car_count}\", total_text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "    \n",
    "        if warning_message:\n",
    "            warning_text_position = (width - 300, 150 + len(zone_car_counts) * 30)\n",
    "            cv2.putText(annotated_frame, \"WARNING: Cars too close!\", warning_text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "            print(warning_message)  \n",
    "    \n",
    "        return annotated_frame\n",
    "    \n",
    "    cap = cv2.VideoCapture(VIDEO)\n",
    "    fps = video_info.fps\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    print(f\"FPS: {fps}\")\n",
    "    print(f\"Resolution: {width}x{height}\")\n",
    "\n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        show = process_frame(frame, int(fps))\n",
    "        fps_monitor.tick()\n",
    "        \n",
    "        # Calculate and display the actual FPS\n",
    "        actual_fps = 1 / (time.time() - start_time)\n",
    "        fps_text = f\"FPS: {actual_fps:.2f}\"\n",
    "        cv2.putText(show, fps_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the processed frame\n",
    "        cv2.imshow(\"Counting - Speed Estimation\", show)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        processing_time = time.time() - start_time\n",
    "        sleep_time = max(1/fps - processing_time, 0)\n",
    "        time.sleep(sleep_time)\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNFxa449UgadN8OXq4W4yJ6",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
